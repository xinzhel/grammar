{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.extract_claims import extract_claim\n",
    "import json\n",
    "\n",
    "\n",
    "linguistic_control = \"short\"\n",
    "# load results\n",
    "with open(f'eval_data/results_{linguistic_control}_queries.json') as f:\n",
    "    results = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity with spec: Project_select1\n",
      "Entity with spec: Project_select2\n",
      "Entity with spec: Employee_select1\n",
      "Entity with spec: Company\n",
      "Entity with spec: Employee_select2\n"
     ]
    }
   ],
   "source": [
    "for entity_with_spec in results:\n",
    "    print(f\"Entity with spec: {entity_with_spec}\")\n",
    "    for result in results[entity_with_spec]:\n",
    "        text = \"Query:\" + result['query'] + \"\\n\" + \"Answer:\" + result['gpt_response']\n",
    "        claims_in_response = extract_claim([text])\n",
    "        result['claims_in_gpt_response'] = eval(claims_in_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAS-Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from indo_eval.llm import OpenAILLMAgent, AnyOpenAILLM, gpt4_llm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RagasFact:\n",
    "    def __init__(self, valid_gen=['Yes', 'No', 'Null']) -> None:\n",
    "        self.num_claims = 0\n",
    "        self.num_eval_instances = 0\n",
    "        self.failure_gen = 0\n",
    "        self.llm = gpt4_llm\n",
    "        self.valid_gen = valid_gen\n",
    "    \n",
    "    def valid_gen_str(self):\n",
    "        return ', '.join(['\"'+s+'\"' for s in self.valid_gen[:-1]]) + ' and \"' + self.valid_gen[-1] + '\"'\n",
    "\n",
    "    def eval(self, context: str, facts: List[str]):\n",
    "        results = []\n",
    "        \n",
    "        for fact in facts:\n",
    "            self.num_claims += 1\n",
    "            verdict = None\n",
    "            while verdict not in self.valid_gen:\n",
    "                prompt = \"\"\"Natural language inference. Use only {valid_gen_str} as verdict. \n",
    "                \\ncontext: {context} \n",
    "                \\nstatement: {statement}\n",
    "                \\nverdict:\"\"\".format(valid_gen_str=self.valid_gen_str(), context=context, statement=fact)\n",
    "                verdict = self.llm(prompt)\n",
    "                if verdict not in self.valid_gen:\n",
    "                    self.failure_gen += 1\n",
    "            results.append(verdict )\n",
    "        self.num_eval_instances += 1\n",
    "        \n",
    "        return results, self.compute_faithfulness_score(results)\n",
    "    \n",
    "    def compute_faithfulness_score(self, output: List[str]):\n",
    "        assert self.valid_gen == [\"Yes\", \"No\", \"Null\"]\n",
    "        total_null = sum(1 for validation in output if validation == \"Null\")\n",
    "        # check the verdicts and compute the score\n",
    "        verdict_score_map = {\"Yes\": 1, \"No\": 0}\n",
    "        faithful_statements = sum(\n",
    "            verdict_score_map.get( validation)\n",
    "            for validation in output if validation != \"Null\"\n",
    "        )\n",
    "        \n",
    "        num_statements = len(output) -  total_null\n",
    "        if num_statements:\n",
    "            score = faithful_statements  / num_statements\n",
    "        else:\n",
    "            score = None\n",
    "       \n",
    "        return score\n",
    "    \n",
    "    def reset(self):\n",
    "        self.failure_gen = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity with spec: Project_select1\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Null']\n",
      "['Yes', 'Yes', 'Null']\n",
      "['Yes']\n",
      "['Yes', 'Yes', 'Yes']\n",
      "['Yes', 'Yes']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Null']\n",
      "['Null', 'Null']\n",
      "['Yes', 'Yes', 'Null', 'Null']\n",
      "['Yes', 'Yes', 'Null']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\n",
      "['Yes']\n",
      "['Yes', 'Yes', 'Null']\n",
      "['Null']\n",
      "['Null']\n",
      "['Yes']\n",
      "Entity with spec: Project_select2\n",
      "['Yes']\n",
      "['Null', 'Null', 'Null']\n",
      "['Null', 'Null', 'Null']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Yes', 'Yes']\n",
      "['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']\n",
      "['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']\n",
      "['Yes', 'Yes']\n",
      "['Null', 'Yes', 'Null', 'Null']\n",
      "['Yes', 'Null']\n",
      "['Yes', 'Yes', 'Yes']\n",
      "Entity with spec: Employee_select1\n",
      "['Yes', 'Yes', 'Yes', 'Null', 'Yes']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Yes', 'Yes']\n",
      "['Yes', 'Yes', 'No']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Yes', 'No', 'Yes', 'No', 'No', 'No', 'Yes']\n",
      "['Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Null', 'Null', 'No', 'No', 'Null']\n",
      "['Yes']\n",
      "['Yes', 'Null']\n",
      "['Yes']\n",
      "['Yes', 'Null']\n",
      "['Null']\n",
      "['Null']\n",
      "['Yes', 'Yes', 'Yes']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Null']\n",
      "['Yes']\n",
      "['Yes', 'Yes']\n",
      "['Yes', 'Yes', 'Null']\n",
      "['Yes']\n",
      "['Null']\n",
      "['Null']\n",
      "['Null']\n",
      "['Yes', 'Null', 'Null']\n",
      "['Yes', 'Yes']\n",
      "['Null', 'Null', 'Null']\n",
      "['Yes']\n",
      "['Null', 'Null']\n",
      "['Null']\n",
      "Entity with spec: Company\n",
      "['Yes', 'Yes', 'Yes']\n",
      "['Yes', 'Yes', 'Yes', 'Null', 'Yes', 'No', 'Yes']\n",
      "['Yes', 'No', 'Null', 'Yes']\n",
      "['Yes', 'Yes', 'Yes']\n",
      "['Yes', 'Yes']\n",
      "['Yes']\n",
      "['Null']\n",
      "['Yes', 'Yes']\n",
      "['No', 'Null', 'Yes']\n",
      "Entity with spec: Employee_select2\n",
      "Number of queries:  81\n",
      "Failure rate:  0.0\n"
     ]
    }
   ],
   "source": [
    "num_queries = 0\n",
    "ragas_fact = RagasFact()\n",
    "for entity_with_spec in results:\n",
    "    print(f\"Entity with spec: {entity_with_spec}\")\n",
    "    for result in results[entity_with_spec]:\n",
    "        num_queries+=1\n",
    "        # text = \"Query:\" + result['query'] + \"\\n\" + \"Answer:\" + result['gpt_response']\n",
    "        # claims_in_response = extract_claim([text])\n",
    "        if \"raga_faithfulness_score\" in result:\n",
    "            continue\n",
    "        result[\"raga_judgement\"], result[\"raga_faithfulness_score\"] = ragas_fact.eval(result['retrieved_documents'], result['claims_in_gpt_response'])\n",
    "        print(result[\"raga_judgement\"])\n",
    "\n",
    "print(\"Number of queries: \", num_queries)\n",
    "print(\"Failure rate: \", ragas_fact.failure_gen/num_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'eval_data/results_{linguistic_control}_queries_raga.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity with spec: Project_select1\n",
      "Entity with spec: Project_select2\n",
      "Entity with spec: Employee_select1\n",
      "Entity with spec: Company\n",
      "Entity with spec: Employee_select2\n",
      "Number of faithful examples:  58\n",
      "Number of correct examples in faithful examples:  11\n",
      "Percentage of correct examples in faithful examples:  0.1896551724137931\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelfCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: {query}            \n",
      "Answer A: {answer}       \n",
      "Answer B: {stochastic_answer}       \n",
      "Do both answers address the query with equivalent meaning?       \n",
      "Use only \"Yes\" or \"No\" for your evaluation:\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from indo_eval.llm import OpenAILLMAgent, AnyOpenAILLM, gpt4_llm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SelfCheck:\n",
    "    def __init__(self, valid_gen=['Yes', 'No']) -> None:\n",
    "        self.total_gen = 0\n",
    "        self.failure_gen = 0\n",
    "        self.llm = gpt4_llm\n",
    "        self.valid_gen = valid_gen\n",
    "    \n",
    "    def valid_gen_str(self):\n",
    "        return ', '.join(['\"'+s+'\"' for s in self.valid_gen[:-1]]) + ' or \"' + self.valid_gen[-1] + '\"'\n",
    "\n",
    "    def eval(self, answer: str, query: str, stochastic_answers: List[str]):\n",
    "        results = []\n",
    "\n",
    "        for stochastic_answer in stochastic_answers:\n",
    "            while True:\n",
    "                prompt = \"\"\"Query: {query} \\\n",
    "                      \\nAnswer A: {answer} \\\n",
    "                      \\nAnswer B: {stochastic_answer} \\\n",
    "                      \\nDo both answers address the query with equivalent meaning? \\\n",
    "                      \\nUse only \"Yes\" or \"No\" for your evaluation:\"\"\".format( query=query, answer=answer, stochastic_answer=stochastic_answer, valid_gen_str=self.valid_gen_str())\n",
    "                verdict = self.llm(prompt)\n",
    "                self.total_gen += 1\n",
    "                if verdict not in self.valid_gen:\n",
    "                    self.failure_gen += 1\n",
    "                else:\n",
    "                    results.append(verdict)\n",
    "                    break\n",
    "                \n",
    "        assert len(results) == len(stochastic_answers)\n",
    "        return results, self.compute_inconsistency_score(results)\n",
    "    \n",
    "    def compute_inconsistency_score(self, output: List[str]):\n",
    "        assert self.valid_gen == [\"Yes\", \"No\"]\n",
    "        \n",
    "        # check the verdicts and compute the score\n",
    "        verdict_score_map = {\"Yes\": 1, \"No\": 0}\n",
    "        num_cosistent_samples = sum(\n",
    "            verdict_score_map.get( validation)\n",
    "            for validation in output\n",
    "        )\n",
    "        \n",
    "        num_samples = len(output) \n",
    "        if num_samples:\n",
    "            score = 1- (num_cosistent_samples  / num_samples)\n",
    "        else:\n",
    "            score = None\n",
    "       \n",
    "        return score\n",
    "    \n",
    "    def reset(self):\n",
    "        self.failure_gen = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "file_name = f'eval_data/results_{linguistic_control}_queries_selfcheck_N{N}.json'\n",
    "with open(file_name) as f:\n",
    "    results_selfcheck = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity with spec: Project_select1\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Entity with spec: Project_select2\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Exists~\n",
      "Entity with spec: Employee_select1\n",
      "Exists~\n",
      "['No', 'No', 'No']\n",
      "1.0\n",
      "['Yes', 'Yes', 'No']\n",
      "0.33333333333333337\n",
      "['Yes', 'Yes', 'No']\n",
      "0.33333333333333337\n",
      "['Yes', 'No', 'No']\n",
      "0.6666666666666667\n",
      "['Yes', 'Yes', 'No']\n",
      "0.33333333333333337\n",
      "['Yes', 'Yes', 'No']\n",
      "0.33333333333333337\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'No', 'No']\n",
      "0.6666666666666667\n",
      "['No', 'No', 'No']\n",
      "1.0\n",
      "['Yes', 'No', 'Yes']\n",
      "0.33333333333333337\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'No', 'No']\n",
      "0.6666666666666667\n",
      "['Yes', 'Yes', 'No']\n",
      "0.33333333333333337\n",
      "['Yes', 'No', 'Yes']\n",
      "0.33333333333333337\n",
      "['Yes', 'No', 'No']\n",
      "0.6666666666666667\n",
      "['Yes', 'No', 'No']\n",
      "0.6666666666666667\n",
      "['No', 'No', 'Yes']\n",
      "0.6666666666666667\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['No', 'Yes', 'Yes']\n",
      "0.33333333333333337\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['No', 'Yes', 'Yes']\n",
      "0.33333333333333337\n",
      "['No', 'No', 'No']\n",
      "1.0\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'No', 'Yes']\n",
      "0.33333333333333337\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "Entity with spec: Company\n",
      "Entity with spec: Employee_select2\n",
      "['No', 'Yes', 'No']\n",
      "0.6666666666666667\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "['Yes', 'No', 'No']\n",
      "0.6666666666666667\n",
      "['No', 'Yes', 'No']\n",
      "0.6666666666666667\n",
      "['Yes', 'No', 'Yes']\n",
      "0.33333333333333337\n",
      "['Yes', 'No', 'Yes']\n",
      "0.33333333333333337\n",
      "['No', 'Yes', 'No']\n",
      "0.6666666666666667\n",
      "['Yes', 'Yes', 'Yes']\n",
      "0.0\n",
      "Failure rate:  0.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "selfcheck = SelfCheck()\n",
    "for entity_with_spec in results_selfcheck:\n",
    "    print(f\"Entity with spec: {entity_with_spec}\")\n",
    "    for result in results_selfcheck[entity_with_spec]:\n",
    "\n",
    "        if \"selfcheck_inconsistency_score\" not in result:\n",
    "            time.sleep(5)  \n",
    "            result[\"selfcheck_judgement\"], result[\"selfcheck_inconsistency_score\"] = selfcheck.eval(result['gpt_response'], result['query'], result[\"stochastic_answers\"])\n",
    "            print(result[\"selfcheck_judgement\"])\n",
    "            print(result[\"selfcheck_inconsistency_score\"])\n",
    "        else:\n",
    "            print('Exists~')\n",
    "if selfcheck.total_gen:\n",
    "    print(\"Failure rate: \", selfcheck.failure_gen/selfcheck.total_gen)\n",
    "with open(file_name, 'w') as f:\n",
    "    json.dump(results_selfcheck, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Reliability Of Reference-free Evaluation Protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity with spec: Project_select1\n",
      "Entity with spec: Project_select2\n",
      "Entity with spec: Employee_select1\n",
      "Entity with spec: Company\n",
      "Entity with spec: Employee_select2\n",
      "Number of examples evaluated as correct predictions:  58\n",
      "Number of correctly-evaluated examples:  11\n",
      "Percentage of correctly-evaluated examples in examples evaluated as correct predictions (Precision):  0.1896551724137931\n",
      "Percentage of correctly-evaluated examples in examples that are correctly predicted (Recall):  0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "def precision_recall_reference_free_eval(results, score_name, score_evaluated_as_correct_pred):\n",
    "    \"\"\"Reference free evaluation of precision and recall\n",
    "    Args:\n",
    "        results: list of dict, each dict has a score_name key\n",
    "        score_name: str, name of the score\n",
    "        score_for_accurate: float, the score indicated that the prediction is accurate\"\"\"\n",
    "    num_correctly_predicted = 0\n",
    "    num_evaluated_as_correct_pred = 0\n",
    "    num_correctly_evaluated_examples = 0\n",
    "    for entity_with_spec in results:\n",
    "        print(f\"Entity with spec: {entity_with_spec}\")\n",
    "        for result in results[entity_with_spec]:\n",
    "            if result[score_name] == score_evaluated_as_correct_pred:\n",
    "                num_evaluated_as_correct_pred += 1\n",
    "            \n",
    "                if result[\"judgement\"] == \"Correct\":\n",
    "                    num_correctly_evaluated_examples += 1\n",
    "\n",
    "            if result[\"judgement\"] == \"Correct\":\n",
    "                num_correctly_predicted += 1\n",
    "    print(\"Number of examples evaluated as correct predictions: \", num_evaluated_as_correct_pred)\n",
    "    print(\"Number of correctly-evaluated examples: \", num_correctly_evaluated_examples)\n",
    "    print(\"Percentage of correctly-evaluated examples in examples evaluated as correct predictions (Precision): \", num_correctly_evaluated_examples/num_evaluated_as_correct_pred)\n",
    "    print(\"Percentage of correctly-evaluated examples in examples that are correctly predicted (Recall): \", num_correctly_evaluated_examples/num_correctly_predicted)\n",
    "\n",
    "with open(f'eval_data/results_{linguistic_control}_queries_raga.json') as f:\n",
    "    results = json.load(f)\n",
    "precision_recall_reference_free_eval(results, score_name=\"raga_faithfulness_score\", score_evaluated_as_correct_pred=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity with spec: Project_select1\n",
      "Entity with spec: Project_select2\n",
      "Entity with spec: Employee_select1\n",
      "Entity with spec: Company\n",
      "Entity with spec: Employee_select2\n",
      "Number of examples evaluated as correct predictions:  48\n",
      "Number of correctly-evaluated examples:  7\n",
      "Percentage of correctly-evaluated examples in examples evaluated as correct predictions (Precision):  0.14583333333333334\n",
      "Percentage of correctly-evaluated examples in examples that are correctly predicted (Recall):  0.5\n"
     ]
    }
   ],
   "source": [
    "file_name = f'eval_data/results_{linguistic_control}_queries_selfcheck_N{N}_scores.json'\n",
    "with open(file_name) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "precision_recall_reference_free_eval(results, score_name=\"selfcheck_inconsistency_score\", score_evaluated_as_correct_pred=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
