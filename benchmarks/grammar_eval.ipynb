{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'grammar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgrammar\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresult\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RAGResult\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgrammar\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag_group\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TaggedGroup \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgrammar\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SemanticsMatch\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'grammar'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from grammar.eval.result import RAGResult\n",
    "from grammar.eval.tag_group import TaggedGroup \n",
    "from grammar.eval.match import SemanticsMatch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Balanced ============\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RAGResult' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m test_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m============ Balanced ============\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m results_long_balanced \u001b[38;5;241m=\u001b[39m \u001b[43minit_eval_results\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mroot_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/eval_results/results_long_balanced.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m results_short_balanced \u001b[38;5;241m=\u001b[39m init_eval_results(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/eval_results/results_short_balanced.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m results_long_balanced, metric_long_balanced \u001b[38;5;241m=\u001b[39m get_eval_results(results_long_balanced, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m'\u001b[39m, root_dir, file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/eval_results/results_long_balanced.json\u001b[39m\u001b[38;5;124m'\u001b[39m, test_mode\u001b[38;5;241m=\u001b[39mtest_mode)\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36minit_eval_results\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m         result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt_response\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt_response\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt_response\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt_response\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mstr\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe gpt_response should be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt_response\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mRAGResult\u001b[49m(query\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m], answer\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m], \\\n\u001b[1;32m     14\u001b[0m                                 gpt_response\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt_response\u001b[39m\u001b[38;5;124m'\u001b[39m], \\\n\u001b[1;32m     15\u001b[0m                                 true_document_ids\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_document_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], query_tag\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_tag\u001b[39m\u001b[38;5;124m'\u001b[39m], \\\n\u001b[1;32m     16\u001b[0m                                 retrieved_document_ids\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretrieved_document_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], \\\n\u001b[1;32m     17\u001b[0m                                 retrieval_judgement\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretrieval_judgement\u001b[39m\u001b[38;5;124m'\u001b[39m], \\\n\u001b[1;32m     18\u001b[0m                                 closed_domain\u001b[38;5;241m=\u001b[39mclosed_domain)\n\u001b[1;32m     19\u001b[0m     eval_results\u001b[38;5;241m.\u001b[39mappend(eval_result)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eval_results\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RAGResult' is not defined"
     ]
    }
   ],
   "source": [
    "def init_eval_results(file_path):\n",
    "    with open(file_path) as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # convert json to dataclass to a new list\n",
    "    eval_results = []    \n",
    "    for result in results:\n",
    "        if isinstance(result['gpt_response'], list):\n",
    "            result['gpt_response'] = result['gpt_response'][0]\n",
    "        assert result['gpt_response'] is None or isinstance(result['gpt_response'], str), f\"The gpt_response should be a string, not {type(result['gpt_response'])}\"\n",
    "            \n",
    "            \n",
    "        eval_result = RAGResult(query=result['query'], answer=result['answer'], \\\n",
    "                                    gpt_response=result['gpt_response'], \\\n",
    "                                    true_document_ids=result['true_document_ids'], query_tag=result['query_tag'], \\\n",
    "                                    retrieved_document_ids=result['retrieved_document_ids'], \\\n",
    "                                    retrieval_judgement=result['retrieval_judgement'], \\\n",
    "                                    closed_domain=closed_domain)\n",
    "        eval_results.append(eval_result)\n",
    "    return eval_results\n",
    "\n",
    "def get_eval_results(eval_results, linguistic_attr, root_dir, file_path, test_mode=False):\n",
    "    \n",
    "    tagged_group = TaggedGroup(eval_results)\n",
    "    semnatics_match = SemanticsMatch.from_file(root_dir=root_dir, verbalize_attrs=linguistic_attr)\n",
    "    if test_mode:\n",
    "        semnatics_match.set_test_mode(True)\n",
    "\n",
    "    for eval_result in tqdm(eval_results):\n",
    "        # sleep for 20 seconds after 9 examples\n",
    "        # if results.index(result) % 9 == 0 and results.index(result) != 0:\n",
    "        #     print(\"Sleeping for 20 seconds\")\n",
    "        #     time.sleep(20)\n",
    "        #     print(\"Waking up\")\n",
    "        eval_result.judge_retrieval_response(tagged_group=tagged_group, method='use_exist')\n",
    "        eval_result.judge_rag_response(semnatics_match)\n",
    "\n",
    "    num_retrieval_failure = sum([result.retrieval_judgement==0 for result in eval_results])\n",
    "    print(f\"Retrieval failed in {num_retrieval_failure} out of {len(eval_results)} examples\")\n",
    "    num_rag_failure = sum([result.judgement==\"Incorrect\" for result in eval_results])\n",
    "    print(f\"RAG failed in {num_rag_failure} out of {len(eval_results)} examples\")\n",
    "    if test_mode:\n",
    "        semnatics_match.save(root_dir=f'{root_dir}', override=True)\n",
    "    # semnatics_match.llm.gpt_usage_record.write_usage(model_name='chatgptk' )\n",
    "\n",
    "    if not test_mode:\n",
    "        # save results\n",
    "        results = [result.asdict() for result in eval_results]\n",
    "        # ensure json serializable\n",
    "        for result in results:\n",
    "            result['true_document_ids'] = list(result['true_document_ids'])\n",
    "            result['retrieved_document_ids'] = list(result['retrieved_document_ids'])\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "    return eval_results, tagged_group\n",
    "\n",
    "root_dir = 'spider'\n",
    "closed_domain = True\n",
    "test_mode = True\n",
    "print('============ Balanced ============')\n",
    "results_long_balanced = init_eval_results(f'{root_dir}/eval_results/results_long_balanced.json')\n",
    "results_short_balanced = init_eval_results(f'{root_dir}/eval_results/results_short_balanced.json')\n",
    "results_long_balanced, metric_long_balanced = get_eval_results(results_long_balanced, 'long', root_dir, file_path=f'{root_dir}/eval_results/results_long_balanced.json', test_mode=test_mode)\n",
    "results_short_balanced, metric_short_balanced = get_eval_results( results_short_balanced, 'short', root_dir, file_path=f'{root_dir}/eval_results/results_short_balanced.json', test_mode=test_mode)\n",
    "print('============ Imbalanced ============')\n",
    "results_long = init_eval_results(f'{root_dir}/eval_results/results_long.json')\n",
    "results_short = init_eval_results(f'{root_dir}/eval_results/results_short.json')\n",
    "results_long, metric_long = get_eval_results(results_long, 'long', root_dir, file_path=f'{root_dir}/eval_results/results_long.json', test_mode=test_mode)\n",
    "results_short, metric_short = get_eval_results(results_short, 'short', root_dir, file_path=f'{root_dir}/eval_results/results_short.json', test_mode=test_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy for short:  0.267515923566879\n",
      "Baseline Accuracy for long:  0.5058365758754864\n",
      "Accuracy (Remove LLM Errors) for short:  0.28662420382165604\n",
      "Accuracy (Remove LLM Errors) for long:  0.5136186770428015\n",
      "Robustness (Removing Gap Examples) for short:  0.9545454545454546\n",
      "Robustness (Removing Gap Examples) for long:  0.9629629629629629\n",
      "Robustness (Removing LLM Errors & Gap Examples) for short:  1.0\n",
      "Robustness (Removing LLM Errors & Gap Examples) for long:  0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# long vs short\n",
    "print('Baseline Accuracy for short: ', metric_short.get_accuracy())\n",
    "print('Baseline Accuracy for long: ', metric_long.get_accuracy())\n",
    "\n",
    "print('Accuracy (Remove LLM Errors) for short: ', metric_short.get_accuracy(for_retrieval=True))\n",
    "print('Accuracy (Remove LLM Errors) for long: ', metric_long.get_accuracy(for_retrieval=True))\n",
    "\n",
    "print('Robustness (Removing Gap Examples) for short: ', metric_short.get_robustness())\n",
    "print('Robustness (Removing Gap Examples) for long: ', metric_long.get_robustness())\n",
    "\n",
    "print('Robustness (Removing LLM Errors & Gap Examples) for short: ', metric_short.get_robustness(for_retrieval=True))\n",
    "print('Robustness (Removing LLM Errors & Gap Examples) for long: ', metric_long.get_robustness(for_retrieval=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy for short:  0.27388535031847133\n",
      "Baseline Accuracy for long:  0.2760084925690021\n",
      "Accuracy (Remove LLM Errors) for short:  0.28662420382165604\n",
      "Accuracy (Remove LLM Errors) for long:  0.2802547770700637\n",
      "Robustness (Removing Gap Examples) for short:  0.9555555555555556\n",
      "Robustness (Removing Gap Examples) for long:  0.9629629629629629\n",
      "Robustness (Removing LLM Errors & Gap Examples) for short:  1.0\n",
      "Robustness (Removing LLM Errors & Gap Examples) for long:  0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# long vs short\n",
    "print('Baseline Accuracy for short: ', metric_short_balanced.get_accuracy())\n",
    "print('Baseline Accuracy for long: ', metric_long_balanced.get_accuracy())\n",
    "\n",
    "print('Accuracy (Remove LLM Errors) for short: ', metric_short_balanced.get_accuracy(for_retrieval=True))\n",
    "print('Accuracy (Remove LLM Errors) for long: ', metric_long_balanced.get_accuracy(for_retrieval=True))\n",
    "\n",
    "print('Robustness (Removing Gap Examples) for short: ', metric_short_balanced.get_robustness())\n",
    "print('Robustness (Removing Gap Examples) for long: ', metric_long_balanced.get_robustness())\n",
    "\n",
    "print('Robustness (Removing LLM Errors & Gap Examples) for short: ', metric_short_balanced.get_robustness(for_retrieval=True))\n",
    "print('Robustness (Removing LLM Errors & Gap Examples) for long: ', metric_long_balanced.get_robustness(for_retrieval=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
